{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0394700a",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c27577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8f2f2",
   "metadata": {},
   "source": [
    "## **Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r'D:\\ExpW_dataset\\images_cropped_224'\n",
    "landmark_root = r'D:\\ExpW_dataset\\landmarks'\n",
    "splits = ['train', 'val', 'test']\n",
    "image_extensions = ('.jpg', '.jpeg', '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71489549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridFERDataset(Dataset):\n",
    "    def __init__(self, image_folder, landmark_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.landmark_folder = landmark_folder\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        \"\"\"Collect all image paths and corresponding landmark paths\"\"\"\n",
    "        for label_name in sorted(os.listdir(self.image_folder)):\n",
    "            class_dir = os.path.join(self.image_folder, label_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            # Assign numeric label\n",
    "            if label_name not in self.class_to_idx:\n",
    "                self.class_to_idx[label_name] = len(self.class_to_idx)\n",
    "\n",
    "            # Find images and corresponding landmarks\n",
    "            image_paths = [\n",
    "                os.path.join(class_dir, f)\n",
    "                for f in os.listdir(class_dir)\n",
    "                if f.lower().endswith(('.png', '.jpg'))\n",
    "            ]\n",
    "            for img_path in image_paths:\n",
    "                filename = os.path.basename(img_path)\n",
    "                landmark_path = os.path.join(\n",
    "                    self.landmark_folder, label_name, filename.replace(\".png\", \".npy\").replace(\".jpg\", \".npy\")\n",
    "                )\n",
    "                if os.path.exists(landmark_path):\n",
    "                    self.samples.append((img_path, landmark_path, self.class_to_idx[label_name]))\n",
    "                else:\n",
    "                    print(f\"Missing landmark: {landmark_path}\")\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} total samples from '{self.image_folder}'.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, lm_path, label = self.samples[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load landmarks\n",
    "        landmarks = torch.from_numpy(np.load(lm_path)).float().flatten()\n",
    "\n",
    "        return image, landmarks, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd40d6",
   "metadata": {},
   "source": [
    "## **FTCS Hybrid ResEmoteNet Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07daf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated SE Block\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# Updated Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 1, stride, 0),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += identity\n",
    "        return F.relu(x)\n",
    "\n",
    "# Visual Backbone\n",
    "class ResEmoteNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, stride=2, padding=3),  # (B, 64, 112, 112)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # (B, 64, 56, 56)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # (B, 128, 28, 28)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # (B, 256, 14, 14)\n",
    "        )\n",
    "        self.se = SEBlock(256)\n",
    "        self.res1 = ResidualBlock(256, 512, 2)  # (B, 512, 7, 7)\n",
    "        self.res2 = ResidualBlock(512, 1024, 2)  # (B, 1024, 4, 4)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.out_features = 1024\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.se(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "# Landmark Branch with optional reconstruction head\n",
    "class LandmarkBranch(nn.Module):\n",
    "    def __init__(self, output_reconstruction=True):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1404, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.output_reconstruction = output_reconstruction\n",
    "        if output_reconstruction:\n",
    "            self.decoder = nn.Sequential(\n",
    "                nn.Linear(256, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 1404)  # Match input dimension for reconstruction\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.output_reconstruction:\n",
    "            decoded = self.decoder(encoded)\n",
    "            return encoded, decoded\n",
    "        else:\n",
    "            return encoded, None\n",
    "\n",
    "\n",
    "# Cross-Modality Fusion (optional)\n",
    "class FusionModule(nn.Module):\n",
    "    def __init__(self, img_dim, lm_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(img_dim + lm_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "    def forward(self, img_feat, lm_feat):\n",
    "        x = torch.cat([img_feat, lm_feat], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Hybrid Model with projection heads for contrastive loss\n",
    "class HybridEmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.visual = ResEmoteNet()\n",
    "        self.landmarks = LandmarkBranch(output_reconstruction=True)\n",
    "        self.fusion = FusionModule(self.visual.out_features, 256)\n",
    "        self.classifier = nn.Linear(128, 7)\n",
    "\n",
    "        # Add projection heads for contrastive loss\n",
    "        self.projection_img = nn.Linear(1024, 128)\n",
    "        self.projection_lm = nn.Linear(256, 128)\n",
    "\n",
    "    def forward(self, img, lm):\n",
    "        img_feat = self.visual(img)                    # (B, 1024)\n",
    "        lm_feat, lm_recon = self.landmarks(lm)         # (B, 256), (B, 936)\n",
    "        fused = self.fusion(img_feat, lm_feat)         # (B, 128)\n",
    "        logits = self.classifier(fused)                # (B, 7)\n",
    "\n",
    "        # Project to same space for cosine contrastive loss\n",
    "        img_proj = self.projection_img(img_feat)       # (B, 128)\n",
    "        lm_proj = self.projection_lm(lm_feat)          # (B, 128)\n",
    "\n",
    "        return logits, img_proj, lm_proj, lm_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e950b",
   "metadata": {},
   "source": [
    "## **Initializing Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Match updated model input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = HybridFERDataset(\n",
    "    image_folder= r'D:\\ExpW_dataset\\images_cropped_224\\train',\n",
    "    landmark_folder= r'D:\\ExpW_dataset\\landmarks\\train',\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "val_dataset = HybridFERDataset(\n",
    "    image_folder= r'D:\\ExpW_dataset\\images_cropped_224\\val',\n",
    "    landmark_folder =r'D:\\ExpW_dataset\\landmarks\\val',\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = HybridFERDataset(\n",
    "    image_folder= r'D:\\ExpW_dataset\\images_cropped_224\\test',\n",
    "    landmark_folder= r'D:\\ExpW_dataset\\landmarks\\test',\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22384ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize everything\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = HybridEmotionModel().to(device)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_contrastive = nn.CosineEmbeddingLoss()\n",
    "criterion_recon = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "start_epoch = 0\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "# Lists to store history\n",
    "train_losses, val_losses, test_losses = [], [], []\n",
    "train_accuracies, val_accuracies, test_accuracies = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8f7d2",
   "metadata": {},
   "source": [
    "## **Checkpoints for Training pausing and resuming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae76aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint if it exists\n",
    "checkpoint_path = 'checkpoint.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_val_acc = checkpoint['best_val_acc']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    test_losses = checkpoint['test_losses']\n",
    "    train_accuracies = checkpoint['train_accuracies']\n",
    "    val_accuracies = checkpoint['val_accuracies']\n",
    "    test_accuracies = checkpoint['test_accuracies']\n",
    "    print(f\"Resumed training from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a092c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks, labels in loader:\n",
    "            images, landmarks, labels = images.to(device), landmarks.to(device), labels.to(device)\n",
    "            logits, _, _, _ = model(images, landmarks)\n",
    "            loss = criterion_ce(logits, labels)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss_sum += loss.item()\n",
    "    return loss_sum / len(loader), correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb92e2",
   "metadata": {},
   "source": [
    "## **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, landmarks, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, landmarks, labels = images.to(device), landmarks.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, img_feat, lm_feat, lm_recon = model(images, landmarks)\n",
    "\n",
    "        loss_ce = criterion_ce(logits, labels)\n",
    "        sim_target = torch.ones(img_feat.size(0)).to(device)\n",
    "        loss_contrastive = criterion_contrastive(img_feat, lm_feat, sim_target)\n",
    "        loss_recon = criterion_recon(lm_recon, landmarks)\n",
    "\n",
    "        total_loss = loss_ce + 0.1 * loss_contrastive + 0.05 * loss_recon\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    test_loss, test_acc = evaluate(test_loader)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), r'D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\models\\expw_best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in val accuracy for {patience_counter} epochs.\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Save checkpoint after every epoch\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'patience_counter': patience_counter,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb3173",
   "metadata": {},
   "source": [
    "## **Plotting Accuracy Convergences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epochs:\", epoch)\n",
    "print(\"Train Losses:\", len(train_losses))\n",
    "print(\"Validation Losses:\", len(val_losses))\n",
    "print(\"Train Accuracies:\", len(train_accuracies))\n",
    "print(\"Validation Accuracies:\", len(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df = pd.DataFrame({\n",
    "    'Epoch': list(range(1, len(train_losses)+1)),\n",
    "    'Train Loss': train_losses,\n",
    "    'Validation Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Validation Accuracy': val_accuracies,\n",
    "})\n",
    "df.to_csv(\"expw_training_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f435ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0723ea",
   "metadata": {},
   "source": [
    "## **Testing Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41372f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = HybridEmotionModel().to(device)\n",
    "model.load_state_dict(torch.load(r'D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\models\\expw_best_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Loss functions\n",
    "criterion_ce = torch.nn.CrossEntropyLoss()\n",
    "criterion_contrastive = torch.nn.CosineEmbeddingLoss()\n",
    "criterion_recon = torch.nn.L1Loss()\n",
    "\n",
    "# Evaluation\n",
    "total, correct = 0, 0\n",
    "loss_ce_sum, loss_contrastive_sum, loss_recon_sum = 0.0, 0.0, 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, landmarks, labels in test_loader:\n",
    "        images, landmarks, labels = images.to(device), landmarks.to(device), labels.to(device)\n",
    "\n",
    "        logits, img_feat, lm_feat, lm_recon = model(images, landmarks)\n",
    "\n",
    "        # Loss components\n",
    "        loss_ce = criterion_ce(logits, labels)\n",
    "        sim_target = torch.ones(img_feat.size(0)).to(device)\n",
    "        loss_contrastive = criterion_contrastive(img_feat, lm_feat, sim_target)\n",
    "        loss_recon = criterion_recon(lm_recon, landmarks)\n",
    "\n",
    "        # Accumulate\n",
    "        loss_ce_sum += loss_ce.item()\n",
    "        loss_contrastive_sum += loss_contrastive.item()\n",
    "        loss_recon_sum += loss_recon.item()\n",
    "\n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Final averaged losses\n",
    "avg_loss_ce = loss_ce_sum / len(test_loader)\n",
    "avg_loss_contrastive = loss_contrastive_sum / len(test_loader)\n",
    "avg_loss_recon = loss_recon_sum / len(test_loader)\n",
    "test_accuracy = correct / total\n",
    "\n",
    "print(f\"\\n Full Test Results:\")\n",
    "print(f\"Test CrossEntropy Loss: {avg_loss_ce:.4f}\")\n",
    "print(f\"Test CosineEmbedding (Contrastive) Loss: {avg_loss_contrastive:.4f}\")\n",
    "print(f\"Test Landmark Reconstruction (L1) Loss: {avg_loss_recon:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0774e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\inference_pipeline\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\logging\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\models\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\utils\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\__init__.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\\face_detector.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\\hybrid_model.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\\landmark_extraction.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\\predictor.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\components\\__init__.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\inference_pipeline\\main.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\inference_pipeline\\__init__.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\logging\\logger.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\logging\\__init__.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\models\\FER_best_model.pth\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\models\\__init__.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\utils\\id_generator.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\utils\\image_utils.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\utils\\save_data.py\n",
      "D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src\\utils\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def print_directory_structure(root_dir):\n",
    "    root_path = Path(root_dir)\n",
    "    \n",
    "    for path in root_path.rglob('*'):\n",
    "        print(path)\n",
    "\n",
    "# Example usage:\n",
    "root_directory = r'D:\\IIT BBS\\Intern works\\Flasho tech\\expression_identifier\\src'\n",
    "print_directory_structure(root_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69201e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
